# Learning to learn without gradient descent by gradient descent

* **Author**: Yutian Chen、 MatthewW. Hoffman 、 Sergio G´omez Colmenarejo 、 Misha Denil *et al*.
* **Abstract**: We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in thatthey can be used to efficiently optimize a broad range of derivative-free black-box functions, includingGaussian process bandits, simple controlobjectives, global optimization benchmarks andhyper-parameter tuning tasks. Up to the traininghorizon, the learned optimizers learn to tradeoffexploration and exploitation, and comparefavourably with heavily engineered Bayesian optimizationpackages for hyper-parameter tuning.
* **Summary**: The experiments have shown that up to the training horizonthe learned RNN optimizers are able to match the performanceof heavily engineered Bayesian optimization solutions,including Spearmint, SMAC and TPE. The trained RNNs rely on neither heuristics nor hyper-parameters when being deployed as black-box optimizers. The optimizers trained on synthetic functions were able to transfer successfully to a very wide class of black-box functions, associated with GP bandits, control, global optimization benchmarks, and hyper-parameter tuning. The experiments have also shown that the RNNs are massively faster than other Bayesian optimization methods. Hence, for applications involving a known horizon and
  where speed is crucial, we recommend the use of the RNN optimizers. The parallel version of the algorithm also performed well when tuning the hyper-parameters of an expensive-to-train residual network.
* **Keywords**: Few-shot learning, Meta-learning
* **code**：
* **dataset**：

