# HyperAdam: A Learnable Task-Adaptive Adam for Network Training

* **Author**:ShipengWang, Jian Sun, Zongben Xu
* **Abstract**: Deep neural networks are traditionally trained using humandesigned stochastic optimization algorithms, such as SGD and Adam. Recently, the approach of learning to optimize network parameters has emerged as a promising research topic. However, these learned black-box optimizers sometimes do not fully utilize the experience in human-designed optimizers, therefore have limitation in generalization ability. In this paper, a new optimizer, dubbed as HyperAdam, is proposed that combines the idea of “learning to optimize” and traditional Adam optimizer. Given a network for training, its parameter update in each iteration generated by HyperAdam is an adaptive combination of multiple updates generated by Adam with  varying decay rates . The combination weights and decay rates in HyperAdam are adaptively learned depending on the task. HyperAdam is modeled as a recurrent neural network with AdamCell, WeightCell and StateCell. It is justified to be state-of-the-art for various network training, such as multilayer perceptron, CNN and LSTM.
* **Summary**:In this paper, we proposed a novel optimizer HyperAdam implementing “learning to optimize” inspired by the traditional Adam optimizer and ensemble learning. It adaptively combines the multiple candidate parameter updates generated by Adam with multiple adaptively learned decay rates. Based on this motivation, a carefully designed RNN was proposed for implementing HyperAdam optimizer. It was justified to outperform or  match traditional optimizers such as Adam, SGD and state-of-art learning-based optimizers in diverse networks training tasks.
* **code**：
* **dataset**：MNIST

