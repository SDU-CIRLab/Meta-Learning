# LLR: Learning learning rates by LSTM for training neural networks

* **Author**:Changyong Yu, Xin Qi, Haitao Ma, Xin He, Cuirong Wang, Yuhai Zhao
* **Abstract**:In the training process of the deep neural networks, the learning rate plays an important role in whether the training process can converge and how fast it can achieve converge. In order to ensure convergence, most of the existing optimization methods adopt a multi-stage descending small learning rate which is hand-designed. However, this method converges slowly especially in the early stage of training. Based on this, a learning rate adjustment strategy that can automatically adjust and has a faster speed of loss decline will be helpful to the training of the deep model. In this paper, a dynamic adjustment strategy of learning rate is developed based on the Long Short Term Memory(LSTM) model and the gradients of loss function. This method effectively utilizes the advantages of the LSTM model considering the multistep learning rate as a whole, and generates the learning rate of the current step based on the memory information of the previous learning rate. Three datasets and four architectures are used in the experiments. We applied the learning rate adjustment method to various optimization methods and achieved good results that our method can achieve even smaller loss under the same number of iterations.
* **Summary**:This paper proposes a learning rate adjustment strategy based on LSTM. We consider the correlation between learning rates be- tween multiple iterations. Making full use of the advantages of LSTM in processing serialized data to provide learning rates for each step of neural network training, So that each parameter up- date can minimize the loss. Our method is verified on different datasets and network structures. The experimental results show that the LSTM-based learning rate adjustment strategy achieves faster convergence speed and smaller training errors than other methods. Although the learning rate optimizer performs slightly worse when it is combined with adaptive optimization methods, as the network model becomes more complex, its learning ability is gradually enhanced. In future work, we will continue to explore ways to make it compatible with adaptive Better combination of optimization methods.
* **code**： 
* **dataset**：MNIST、CIFAR10

