# How to train your MAML

* **Author**:Antreas Antoniou、Harrison Edwards、Amos Storkey
* **Abstract**: The field of few-shot learning has recently seen substantial advancements. Most of these advancements came from casting few-shot learning as a meta-learning problem. Model Agnostic Meta Learning or MAML is currently one of the best approaches for few-shot learning via meta-learning. MAML is simple, elegant and very powerful, however, it has a variety of issues, such as being very sensitive to neural network architectures, often leading to instability during training, requiring arduous hyperparameter searches to stabilize training and achieve high generalization and being very computationally expensive at both training and inference times. In this paper, we propose various modifications to MAML that not
  only stabilize the system, but also substantially improve the generalization performance, convergence speed and computational overhead of MAML, which we call MAML++.
* **Summary**: In this paper we delve deep into what makes or breaks the MAML framework and propose multiple ways to automate most of the hyperparameter searching required, improve the generalization error,stabilize and speed up MAML. The resulting approach, called MAML++ sets a new state of the art across all few-shot tasks, across Omniglot and Mini-Imagenet. The results of the approach indicate that learning per-step learning rates, batch normalization parameters and optimizing on perstep target losses appears to be key for fast, highly automatic and strongly generalizable few-shot learning.
* **Keywords**: Few-shot learning, Meta-learning
* **code**：
* **dataset**：Omniglot  and Mini-Imagenet

