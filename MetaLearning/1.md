# Reptile: a Scalable Metalearning Algorithm
* **Author**: Alex Nichol and John Schulman
* **Abstract**: This paper considers metalearning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We present a remarkably simple met- alearning algorithm called Reptile, which learns a parameter initialization that can be ne-tuned quickly on a new task. Reptile works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. Unlike MAML, which also learns an initialization, Reptile doesn't require dierentiating through the optimization process, making it more suitable for optimization problems where many update steps are required. We show that Reptile performs well on some well-established benchmarks for few-shot classication. We provide some theoretical analysis aimed at understanding why Reptile works.
* **Summary**: In metalearning problems, we assume access to a training set of tasks, which we use to train a fast learner. We described a surprisingly simple approach for metalearning, which works by repeatedly optimizing on a single task, and moving the parameter vector towards the parameters learned on that task. This algorithm performs similarly to MAML [FAL17], while being signicantly simpler to implement. We gave two theoretical explanations for why Reptile works. First, by approximating the update with a Taylor series, we showed that the key leading-order term matches the gradient from MAML [FAL17]. This term adjusts the initial weights to maximize the dot product between the gradients of different minibatches on the same task|i.e., it encourages the gradients to generalize between minibatches of the same task. We also provided a second informal argument, which is that Reptile nds a point that is close (in Euclidean distance) to all of the optimal solution manifolds of the training tasks. While this paper studies the metalearning setting, the Taylor series analysis in Section 4.1 may have some bearing on stochastic gradient descent in general. It suggests that when doing stochastic gradient descent, we are automatically performing a MAML-like update that maximizes the generalization between dierent minibatches. This observation partly explains why ne tuning (e.g., from ImageNet to a smaller dataset [ZDGD14]) works well|SGD automatically gives us an initialization that generalizes well to similar tasks. This hypothesis would suggest that joint training plus ne tuning will continue to be a strong baseline for metalearning in various machine learning problems.
* **Keywords**: Few-shot learning, Meta-learning
* **code**：
* **dataset**：Omniglot、MiniImagenet datasets
* **个人想法**：学习神经网络的初始值，和MAML方法类似，同样值得阅读。

