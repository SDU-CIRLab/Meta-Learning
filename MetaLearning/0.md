# Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks
* **Author**: Chelsea Finn、 Pieter Abbeel 、 Sergey Levine 
* **Abstract**: We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta- learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We  demonstrate that this approach leads to state-of-the-art performance on two fewshot image classification benchmarks, produces good results on  few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.
* **Summary**: We introduced a meta-learning method based on learning easily adaptable model parameters through gradient descent. Our approach has a number of benefits. It is simple and does not introduce any learned parameters for metalearning. It can be combined with any model representation that is amenable to gradient-based training, and any differentiable objective, including classification, regression, and reinforcement learning. Lastly, since our method merely produces a weight initialization, adaptation can be performed with any amount of data and any number of gradient steps, though we demonstrate state-of-the-art results on classification with only one or five examples per class. We also show that our method can adapt an RL agent using policy gradients and a very modest amount of experience. Reusing knowledge from past tasks may be a crucial ingredient in making high-capacity scalable models, such as deep neural networks, amenable to fast training with small datasets. We believe that this work is one step toward a simple and general-purpose meta-learning technique that can be applied to any problem and any model. Further research in this area can make multitask initialization a standard ingredient in deep learning and reinforcement learning.
* **Keywords**: Few-shot learning, Meta-learning
* **code**：Code for the regression and supervised experiments is at github.com/cbfinn/maml and code for the RL experiments is at github.com/cbfinn/maml_rl
* **dataset**：Omniglot、MiniImagenet datasets
* **个人想法**：学习神经网络的初始值，想法简单，易于理解，是元学习入门必读文章。

